apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: straggler-shield
  namespace: straggler-shield
  labels:
    app.kubernetes.io/name: straggler-shield
spec:
  selector:
    matchLabels:
      app: straggler-shield
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app: straggler-shield
    spec:
      serviceAccountName: straggler-shield-agent

      # nvidia runtime injects driver mounts, nvidia-smi, and device nodes
      # into the container. Without this, CUDA calls and nvidia-smi fail silently.
      runtimeClassName: nvidia

      # Ensure the agent is not preempted by workload pods during node pressure.
      priorityClassName: system-node-critical

      tolerations:
        # Core requirement: the agent must survive on nodes it has just quarantined.
        # Without this toleration, K8s evicts the pod the moment it writes the
        # zombie taint, leaving the node unmonitored for subsequent reboots.
        - key: "sunk.coreweave.com/zombie-quarantine"
          operator: "Exists"
          effect: "NoSchedule"

        # Standard node lifecycle taints. K8s 1.26+ adds not-ready/unreachable
        # automatically to DaemonSet pods, but the others require explicit opt-in.
        - key: "node.kubernetes.io/not-ready"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 300
        - key: "node.kubernetes.io/unreachable"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 300
        - key: "node.kubernetes.io/disk-pressure"
          operator: "Exists"
          effect: "NoSchedule"
        - key: "node.kubernetes.io/memory-pressure"
          operator: "Exists"
          effect: "NoSchedule"
        - key: "node.kubernetes.io/pid-pressure"
          operator: "Exists"
          effect: "NoSchedule"
        - key: "node.kubernetes.io/unschedulable"
          operator: "Exists"
          effect: "NoSchedule"

      # Restrict to nodes that expose NVIDIA GPUs via the device plugin.
      # On CoreWeave, GPU nodes are labeled by the NVIDIA device plugin at startup.
      # Adjust if your cluster uses a different label convention.
      nodeSelector:
        nvidia.com/gpu.present: "true"

      # /tmp is needed by the CUDA runtime and nvidia-smi in some configurations.
      # Backed by memory to avoid writes to the read-only root filesystem.
      volumes:
        - name: tmp
          emptyDir:
            medium: Memory
            sizeLimit: 32Mi

      containers:
        - name: agent
          # Pin to a specific digest in production; :latest is fine for eval.
          image: ghcr.io/justin-oleary/straggler-shield:latest
          imagePullPolicy: Always

          env:
            # NODE_NAME is the only runtime input the agent needs. It is consumed
            # by main.go to scope the node watch and identify itself in logs.
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName

            # Optional threshold overrides. Remove any line to use the compiled default.
            # - name: PULSE_THRESHOLD_MS
            #   value: "500"
            # - name: PULSE_CV_MAX
            #   value: "0.20"
            # - name: P2P_MIN_GBS
            #   value: "5.0"
            # - name: IDLE_TEMP_MAX
            #   value: "70"
            # - name: READY_WINDOW_SECONDS
            #   value: "300"

          resources:
            limits:
              # Requesting a GPU device causes the device plugin to assign one
              # physical GPU to this container and mount /dev/nvidia* accordingly.
              # The agent itself does not consume the full GPU â€” it runs a brief
              # pulse at node join time and sits idle otherwise.
              nvidia.com/gpu: "1"
              cpu: "1"
              memory: "512Mi"
            requests:
              cpu: "100m"
              memory: "64Mi"

          ports:
            - name: metrics
              containerPort: 9090
              protocol: TCP

          livenessProbe:
            httpGet:
              path: /metrics
              port: 9090
            initialDelaySeconds: 15
            periodSeconds: 30
            failureThreshold: 3

          volumeMounts:
            - name: tmp
              mountPath: /tmp

          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            capabilities:
              drop: ["ALL"]

      securityContext:
        runAsNonRoot: true
        # 65534 = nobody. CUDA device access via the device plugin does not
        # require root. If nvidia-smi fails with permission errors in your
        # specific cluster configuration, set runAsUser: 0 here.
        runAsUser: 65534
        runAsGroup: 65534
